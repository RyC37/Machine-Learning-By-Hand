{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from load_mnist_data import load_data_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\" \n",
    "    This is a class for neural network (NN). \n",
    "      \n",
    "    Attributes: \n",
    "        num_layers (int): The number of layers of NN. \n",
    "        sizes (array): The structure of NN, represent number of neuron in each layer. \n",
    "        biases: The bias of NN.\n",
    "        weights: The weights of NN.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes):\n",
    "        '''Initialize attributes of the neural network based on network structure.\n",
    "        \n",
    "        Parameters:\n",
    "        sizes (array): The structure of neural network.\n",
    "        '''\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # Input layer has no bias, so start with the 2nd layer (sizes[1:])\n",
    "        # The shape of bias coef in each layer is a vertical vector\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        # The shape of weights between each two layers are:\n",
    "        # The row number is the size of latter layer\n",
    "        # The column number is the size of previous number\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1],sizes[1:])]\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        '''Sigmoid transformation\n",
    "        \n",
    "        Parameters:\n",
    "        z (array): The vector to be converted, elementwise.\n",
    "        '''\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        '''The derivative of sigmoid function\n",
    "        \n",
    "        Parameters:\n",
    "        z (array): The vector to be converted, elementwise.\n",
    "        '''\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        '''This function implements stochastic gradient descent\n",
    "        \n",
    "        Parameters:\n",
    "        training_data (zip/list): Training data, containing x and y.\n",
    "        epochs (int): The number of training iterations.\n",
    "        mini_batch_size (int): The sample size for each batch in SGD.\n",
    "        eta (float): The learning rate.\n",
    "        test_data (zip/list): Test data, containing x and y.\n",
    "        '''\n",
    "        training_data = list(training_data)\n",
    "        test_data = list(test_data)\n",
    "        n = len(training_data)\n",
    "        for i in range(epochs):\n",
    "            # Shuffle to make the processes 'stochastic'\n",
    "            np.random.shuffle(training_data)\n",
    "            # Split data into mini batches\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            # Update weights and biases by mini batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            # Test prediction performance if test data available\n",
    "            if test_data:\n",
    "                eval_res = self.evaluate(test_data)\n",
    "                print('Epotch %d: %f' % (i+1, eval_res))\n",
    "            else:\n",
    "                print('Epotch %d complete' % i+1)\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        '''Update model's weights and biases using the results of \n",
    "        backpropagate algorithm, for each mini batch of data\n",
    "        \n",
    "        Parameters:\n",
    "        mini_batch (list): One batch of training data.\n",
    "        eta (float): The learning rate.\n",
    "        '''\n",
    "        # Initiate nabla weights and nabla biases, \n",
    "        # in a way they have the same shape with weights and biases\n",
    "        # fill with 0\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Update nabla weights and nabla biases\n",
    "        for x, y in mini_batch:\n",
    "            # Use backpropagate to calculate the derivative of cost function of bias,\n",
    "            # and the derivative of the cost function of weight. \n",
    "            # And take the average.\n",
    "            delta_nabla_b, delta_nabla_w = self.back_prop(x,y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b,delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        mini_batch_size = len(mini_batch)\n",
    "        # Average the sum of derivatives and update weights and biases\n",
    "        self.biases = [b - eta/mini_batch_size * db for b, db in zip(self.biases, nabla_b)]\n",
    "        self.weights = [w - eta/mini_batch_size * dw for w, dw in zip(self.weights, nabla_w)]\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        '''Calculate the activiations and zs for the NN\n",
    "        \n",
    "        Parameters:\n",
    "        x (array): Input value.\n",
    "        \n",
    "        Output:\n",
    "        activations (array): The activiation value of each neuron.\n",
    "        zs (array): The z value of each neuron.\n",
    "        '''\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        a = np.array(x)\n",
    "            \n",
    "        for i in range(self.num_layers-1):\n",
    "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
    "            a = self.sigmoid(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "        return activations, zs\n",
    "    \n",
    "    def back_prop(self, x, y):\n",
    "        '''Calculate the derivative of the cost function of weights\n",
    "        and the derivative of the cost function of bias\n",
    "        \n",
    "        Parameters:\n",
    "        x (array): Features\n",
    "        y (array): Labels\n",
    "        '''\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Feed forward to calculate z and activiation\n",
    "        activations, zs = self.feed_forward(x)\n",
    "        # Updata weight and bias of last layer\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Update weight and bias of previous layer\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-(l-1)].transpose(), delta) * sp\n",
    "            # Update weights and bias\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-(l+1)].transpose())\n",
    "        return nabla_b, nabla_w\n",
    "        \n",
    "    def cost_derivative(self, activiation_output, y):\n",
    "        '''Calculate the derivative of cost function\n",
    "        \n",
    "        Parameters:\n",
    "        activiation_output (array): The predicted result\n",
    "        y (array): The label.\n",
    "        \n",
    "        Output:\n",
    "        Derivative of cost function in array.\n",
    "        '''\n",
    "        return activiation_output - y\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        '''Evaluate the precition accuracy\n",
    "        \n",
    "        Parameters:\n",
    "        test_data (array)\n",
    "        '''\n",
    "        count = 0\n",
    "        n = len(test_data)\n",
    "        for x, y in test_data:\n",
    "            activations, zs = self.feed_forward(x)\n",
    "            y_pred = np.argmax(activations[-1])\n",
    "            if y_pred == y:\n",
    "                count += 1\n",
    "        return 1.0*count/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784,100,32,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epotch 1: 0.903800\n",
      "Epotch 2: 0.929800\n",
      "Epotch 3: 0.934100\n",
      "Epotch 4: 0.939600\n",
      "Epotch 5: 0.942300\n",
      "Epotch 6: 0.946800\n",
      "Epotch 7: 0.948700\n",
      "Epotch 8: 0.950600\n",
      "Epotch 9: 0.952000\n",
      "Epotch 10: 0.952900\n",
      "Epotch 11: 0.950900\n",
      "Epotch 12: 0.954500\n",
      "Epotch 13: 0.952500\n",
      "Epotch 14: 0.955300\n",
      "Epotch 15: 0.956400\n",
      "Epotch 16: 0.956400\n",
      "Epotch 17: 0.955800\n",
      "Epotch 18: 0.957400\n",
      "Epotch 19: 0.956200\n",
      "Epotch 20: 0.957300\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 20, 20, 3, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8388"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate(list(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
